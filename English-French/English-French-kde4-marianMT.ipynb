{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08aebd17",
   "metadata": {},
   "source": [
    "# English to French translation using the kde4 dataset and marianMT tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05355c54",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85411fce",
   "metadata": {},
   "source": [
    "The dataset is downloaded directly from the hugging face library interface using the 'datasets' library.</br>\n",
    "Once downloaded the dataset will be present in the cache memory of the notebook and can be accessed for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def1dfd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\kde4\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac (last modified on Sun Apr 17 23:34:11 2022) since it couldn't be found locally at kde4., or remotely on the Hugging Face Hub.\n",
      "Using custom data configuration en-fr-lang1=en,lang2=fr\n",
      "Reusing dataset kde4 (C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c914b7f5ea36426bb21e579f76781ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading kde4 dataset [en-english; fr-french]\n",
    "from datasets import load_dataset, load_metric\n",
    "raw_data = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36116b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac\\cache-496be247a58b47c1.arrow and C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac\\cache-2c0faebb61cdd12e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = raw_data[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d6465",
   "metadata": {},
   "source": [
    "'test' key can be renamed as 'validation' for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be390a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data[\"validation\"] = split_data.pop(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12c1ee",
   "metadata": {},
   "source": [
    "Let us look at one instance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e449ecb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Default to expanded threads',\n",
       " 'fr': 'Par défaut, développer les fils de discussion'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91f86c",
   "metadata": {},
   "source": [
    "### Defining the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0acc93",
   "metadata": {},
   "source": [
    "Pre-defined tokenizer of the pre-trained \"opus-mt-en-fr\" model by Helsinki NLP is used to tokenize the text in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb4b97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b30de35b634b948c6af0547aecaf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e26b64eb5ec43f99d8dc0319ea900c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be4cd66d3e64076a2f53192586a37ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df76e460b434913a6e836d89671f3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/784k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d70ffc031347318b9295b1dffa5f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_marianMT = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_marianMT,use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3755ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unlike t5 transformer , marian MT does not require an aciton prefix\n",
    "prefix = \"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d4222",
   "metadata": {},
   "source": [
    "### Defining the pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c4ca746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer hugging face documentations for language codes\n",
    "source_language = \"en\"\n",
    "target_language = \"fr\"\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess(instances):\n",
    "   input = [prefix + i[source_language] for i in instances[\"translation\"]]\n",
    "   target = [i[target_language] for i in instances[\"translation\"]]\n",
    "   tokenized_inputs = tokenizer(input, max_length=max_input_length, truncation=True)\n",
    "   # Setup the tokenizer for target\n",
    "   with tokenizer.as_target_tokenizer():\n",
    "       label = tokenizer(target, max_length=max_target_length, truncation=True)\n",
    "   tokenized_inputs[\"labels\"] = label[\"input_ids\"]\n",
    "   return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755bb2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d17c55908f94ddd83cdc1b740d6a2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/190 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88caf9684ae644cd95df217608725a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying the pre processing on the entire dataset\n",
    "tokenized_datasets = split_data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c4c6c",
   "metadata": {},
   "source": [
    "### Creating subsets of the dataset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e77da716",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb363b5",
   "metadata": {},
   "source": [
    "### Using the 'marianMT' pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1572ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab47101be1c4797972cc9d8c571589c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_marianMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b93d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "#defining training attributes\n",
    "args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   evaluation_strategy = \"epoch\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=batch_size,\n",
    "   per_device_eval_batch_size=batch_size,\n",
    "   weight_decay=0.01,\n",
    "   save_total_limit=3,\n",
    "   num_train_epochs=1,\n",
    "   predict_with_generate=True   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dfd7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad inputs and label them\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30684c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "meteor = load_metric('meteor')\n",
    "\n",
    "#customizing compute_metrics function to display bleu score, mean prediction length and meteor score\n",
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "   if isinstance(preds, tuple):\n",
    "       preds = preds[0]\n",
    "    \n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   # Replacing -100 in the labels as they are not needed and cannot be decoded\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "   \n",
    "   decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "   decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "   \n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "   meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "   prediction_length = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "   \n",
    "   result = {'bleu' : result['score']}\n",
    "   result[\"gen_len\"] = np.mean(prediction_length)\n",
    "   result[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "   result = {x: round(y, 4) for x, y in result.items()}\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3508d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training object with customized parameters\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model,\n",
    "   args,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=eval_dataset,\n",
    "   data_collator=data_collator,\n",
    "   tokenizer=tokenizer,\n",
    "   compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38dd3429",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation, id. If translation, id are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 48:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.508310</td>\n",
       "      <td>42.767200</td>\n",
       "      <td>14.755000</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation, id. If translation, id are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=1.3830310882083954, metrics={'train_runtime': 2945.3346, 'train_samples_per_second': 0.34, 'train_steps_per_second': 0.021, 'total_flos': 14379262672896.0, 'train_loss': 1.3830310882083954, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model using train function\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364a063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
