{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3e5565",
   "metadata": {},
   "source": [
    "# English to French translation using the kde4 dataset and t5 tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589f668",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8701046",
   "metadata": {},
   "source": [
    "The dataset is downloaded directly from the hugging face library interface using the 'datasets' library.</br>\n",
    "Once downloaded the dataset will be present in the cache memory of the notebook and can be accessed for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def1dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241f4b0c045547049fdf97e35643555a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b73095ea5ed4e33bf055ee59bd27d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-fr-lang1=en,lang2=fr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset kde4/en-fr (download: 6.72 MiB, generated: 24.46 MiB, post-processed: Unknown size, total: 31.18 MiB) to C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d115593a94b7683680facb548e84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/210173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset kde4 downloaded and prepared to C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70a5fc9a34545f7b024fdb3eb421407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download the kde4 dataset [en-english; fr-french]\n",
    "from datasets import load_dataset, load_metric\n",
    "raw_data = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abe56c",
   "metadata": {},
   "source": [
    "The dataset is available in a single split and needs to be split to create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ee1656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = raw_data[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1981c97",
   "metadata": {},
   "source": [
    "'test' key can be renamed as 'validation' for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee08afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data[\"validation\"] = split_data.pop(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1ad86",
   "metadata": {},
   "source": [
    "Let us look at one instance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261cb323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Default to expanded threads',\n",
       " 'fr': 'Par défaut, développer les fils de discussion'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84520090",
   "metadata": {},
   "source": [
    "### Defining the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb19cf3",
   "metadata": {},
   "source": [
    "Pre-defined tokenizer of the pre-trained \"t5-small\" model is used to tokenize the text in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb4b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t5 = \"t5-small\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_t5,use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3755ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t5 transformer models require a prefix indicating the action to be performed on the input provided.\n",
    "prefix = \"translate English to French:\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b47d6",
   "metadata": {},
   "source": [
    "### Defining the pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c4ca746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer hugging face documentations for language codes\n",
    "source_language = \"en\"\n",
    "target_language = \"fr\"\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess(instances):\n",
    "   input = [prefix + i[source_language] for i in instances[\"translation\"]]\n",
    "   target = [i[target_language] for i in instances[\"translation\"]]\n",
    "   tokenized_inputs = tokenizer(input, max_length=max_input_length, truncation=True)\n",
    "   # Setup the tokenizer for target\n",
    "   with tokenizer.as_target_tokenizer():\n",
    "       label = tokenizer(target, max_length=max_target_length, truncation=True)\n",
    "   tokenized_inputs[\"labels\"] = label[\"input_ids\"]\n",
    "   return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755bb2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1388adbc1f9346d3bbe517d967f1303e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/190 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40f55c7f4714efbb9e40b3cdd933702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying the pre processing on the entire dataset\n",
    "tokenized_datasets = split_data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a1ac1",
   "metadata": {},
   "source": [
    "### Creating subsets of the dataset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e77da716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac\\cache-dede3008ff233b0c.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6dd42",
   "metadata": {},
   "source": [
    "### Using the 't5-small' pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1572ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b93d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "#defining training attributes\n",
    "args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   evaluation_strategy = \"epoch\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=batch_size,\n",
    "   per_device_eval_batch_size=batch_size,\n",
    "   weight_decay=0.01,\n",
    "   save_total_limit=3,\n",
    "   num_train_epochs=1,\n",
    "   predict_with_generate=True   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dfd7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad inputs and label them\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30684c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "meteor = load_metric('meteor')\n",
    "\n",
    "#customizing compute_metrics function to display bleu score, mean prediction length and meteor score\n",
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "   if isinstance(preds, tuple):\n",
    "       preds = preds[0]\n",
    "    \n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   # Replacing -100 in the labels as they are not needed and cannot be decoded\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "   \n",
    "   decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "   decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "   \n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "   meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "   prediction_length = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "   \n",
    "   result = {'bleu' : result['score']}\n",
    "   result[\"gen_len\"] = np.mean(prediction_length)\n",
    "   result[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "   result = {x: round(y, 4) for x, y in result.items()}\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3508d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training object with customized parameters\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model,\n",
    "   args,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=eval_dataset,\n",
    "   data_collator=data_collator,\n",
    "   tokenizer=tokenizer,\n",
    "   compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38dd3429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, translation. If id, translation are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Sharanya Manohar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 12:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.037610</td>\n",
       "      <td>10.712800</td>\n",
       "      <td>11.035000</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, translation. If id, translation are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=2.308563716827877, metrics={'train_runtime': 734.8737, 'train_samples_per_second': 1.361, 'train_steps_per_second': 0.086, 'total_flos': 16744318500864.0, 'train_loss': 2.308563716827877, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model using train function\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364a063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
