{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a16fb9b",
   "metadata": {},
   "source": [
    "# English to Dutch translation using the opus book dataset and t5 tranformer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf4edf56",
   "metadata": {},
   "source": [
    "### Loading the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8d500d8",
   "metadata": {},
   "source": [
    "The dataset is downloaded directly from the hugging face library interface using the 'datasets' library. </br>\n",
    "Once downloaded the dataset will be present in the cache memory of the notebook and can be accessed for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061910aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset opus_books/en-nl (download: 3.56 MiB, generated: 9.80 MiB, post-processed: Unknown size, total: 13.36 MiB) to C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\datasets\\opus_books\\en-nl\\1.0.0\\e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed81a278932e428893c1a7e39d284c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/38652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset opus_books downloaded and prepared to C:\\Users\\Sharanya Manohar\\.cache\\huggingface\\datasets\\opus_books\\en-nl\\1.0.0\\e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1921fb7af7894d999b92af1e84ae120d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 38652\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading opus_books dataset \n",
    "from datasets import load_dataset, load_metric\n",
    "raw_data = load_dataset(\"opus_books\", \"en-nl\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7fb0b",
   "metadata": {},
   "source": [
    "The dataset is available in a single split and needs to be split to create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53781f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 34786\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 3866\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = raw_data[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734769b",
   "metadata": {},
   "source": [
    "'test' key can be renamed as 'validation' for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0751f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data[\"validation\"] = split_data.pop(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12efde",
   "metadata": {},
   "source": [
    "Let us look at one instance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723bb1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'That was a good time.\"', 'nl': 'Dat was een goede tijd.\"'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83e6fb",
   "metadata": {},
   "source": [
    "### Defining the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb27ae0",
   "metadata": {},
   "source": [
    "Pre-defined tokenizer of the pre-trained \"t5-small\" model is used to tokenize the text in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cfa4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t5 = \"t5-small\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_t5,use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82fae770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t5 transformer models require a prefix indicating the action to be performed on the input provided.\n",
    "prefix = \"translate English to Dutch:\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e79db",
   "metadata": {},
   "source": [
    "### Defining the pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5eb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer hugging face documentations for language codes\n",
    "source_language = \"en\"\n",
    "target_language = \"nl\"\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess(instances):\n",
    "   input = [prefix + i[source_language] for i in instances[\"translation\"]]\n",
    "   target = [i[target_language] for i in instances[\"translation\"]]\n",
    "   tokenized_inputs = tokenizer(input, max_length=max_input_length, truncation=True)\n",
    "   # Setup the tokenizer for target\n",
    "   with tokenizer.as_target_tokenizer():\n",
    "       label = tokenizer(target, max_length=max_target_length, truncation=True)\n",
    "   tokenized_inputs[\"labels\"] = label[\"input_ids\"]\n",
    "   return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8bc9d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f3802c4f1f42e596c4f2f95e03d44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03c32e0477f4476936533db51b43719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying the pre processing on the entire dataset\n",
    "tokenized_datasets = split_data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768ef5d",
   "metadata": {},
   "source": [
    "### Creating subsets of the dataset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28e15ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f262689",
   "metadata": {},
   "source": [
    "### Using the 't5-small' pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e2a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8915416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "#defining training attributes\n",
    "args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   evaluation_strategy = \"epoch\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=batch_size,\n",
    "   per_device_eval_batch_size=batch_size,\n",
    "   weight_decay=0.01,\n",
    "   save_total_limit=3,\n",
    "   num_train_epochs=1,\n",
    "   predict_with_generate=True   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f740fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad inputs and label them\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcdad690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Sharanya\n",
      "[nltk_data]     Manohar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "meteor = load_metric('meteor')\n",
    "\n",
    "#customizing compute_metrics function to display bleu score, mean prediction length and meteor score\n",
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "   if isinstance(preds, tuple):\n",
    "       preds = preds[0]\n",
    "    \n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   # Replacing -100 in the labels as they are not needed and cannot be decoded\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "   \n",
    "   decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "   decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "   \n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "   meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "   prediction_length = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "   \n",
    "   result = {'bleu' : result['score']}\n",
    "   result[\"gen_len\"] = np.mean(prediction_length)\n",
    "   result[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "   result = {x: round(y, 4) for x, y in result.items()}\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c60dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training object with customized parameters\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model,\n",
    "   args,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=eval_dataset,\n",
    "   data_collator=data_collator,\n",
    "   tokenizer=tokenizer,\n",
    "   compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52db2744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, translation. If id, translation are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 18:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.019485</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>17.109000</td>\n",
       "      <td>0.062300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, translation. If id, translation are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=4.33593992203001, metrics={'train_runtime': 1143.1448, 'train_samples_per_second': 0.875, 'train_steps_per_second': 0.055, 'total_flos': 27013377687552.0, 'train_loss': 4.33593992203001, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model using train function\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72ab1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
